# run1.py
- **transforms.Compose**: A function to chain multiple image transformation operations together. Here, `transforms.Resize((224, 224))` resizes the image to 224x224 pixels, and `transforms.ToTensor()` converts the image to a PyTorch tensor.
- **ImageFolder**: A dataset class for loading images from a directory where images are arranged in class-specific subdirectories.
- **DataLoader**: A PyTorch class that provides an efficient way to iterate over datasets.
- **torch.nn**: A PyTorch module containing neural network layers and loss functions.
- **torch.optim**: A PyTorch module containing optimization algorithms.
- **numpy**: A library for numerical computations.
- **tqdm**: A library to display progress bars.


---

# CSWin Transformer Documentation

## Overview
This document describes the CSWin Transformer model, a type of vision transformer that leverages cross-shaped windows to process images more effectively. The implementation includes modifications such as SSF (Spatial-Spectral Fusion) and CBAM (Convolutional Block Attention Module) for enhanced feature extraction capabilities.

## Modules and Functions

### 1. Mlp Class
Description: This class implements a Multilayer Perceptron (MLP) used in transformer blocks. It contains two linear transformations with a dropout and activation in between.
Parameters:
- in_features: Input feature dimension.
- hidden_features: Hidden layer feature dimension. Defaults to the input feature dimension if not specified.
- out_features: Output feature dimension. Defaults to the input feature dimension if not specified.
- act_layer: Activation function, defaulting to GELU.
- drop: Dropout rate.

### 2. LePEAttention Class
Description: Implements Locally-enhanced Positional Encoding (LePE) attention mechanism, which aims to improve positional encoding with local enhancements.
Parameters:
- dim: Dimension of the feature vectors.
- resolution: Resolution of the input feature map.
- idx: Index to determine the splitting strategy.
- split_size: Size to split the attention window.
- dim_out: Output dimension, defaults to input dimension.
- num_heads: Number of attention heads.
- attn_drop: Dropout rate for the attention weights.
- proj_drop: Dropout rate for the output projection.
- qk_scale: Scaling factor for the query-key dot product, defaults to the inverse square root of the head dimension.
The LePEAttention (Locally-enhanced Positional Encoding Attention) mechanism is a variant of the self-attention mechanism used in transformer models, specifically designed to incorporate enhanced positional information. Hereâ€™s a simplified explanation of how it operates:

### 1. Positional Encoding Enhancement
- Unlike standard transformers that use global positional encodings, LePE modifies the positional encodings locally within sub-regions of the input. This approach aims to make the positional encodings more relevant and useful for vision tasks, where local spatial relationships can be critical.

### 2. Setup and Configuration
- Input: The mechanism takes an input feature map (e.g., from an image split into patches).
- Configuration: It is configured with parameters such as the number of heads in multi-head attention, the dimensions of each head, and the split size, which defines how the input is partitioned into smaller regions or windows.

### 3. Windowing
- The input feature map is divided into windows according to the specified split_size. This windowing step is crucial because it confines the computation of attention to smaller local regions, reducing computational complexity and allowing the model to focus more on local features.

### 4. Query, Key, Value Computations
- Within each window, the feature vectors are used to compute queries (Q), keys (K), and values (V) using linear transformations. Optionally, a bias can be added to these transformations (qkv_bias).

### 5. Scaled Dot-Product Attention
- The dot products between the queries and keys are computed, which represent the attention scores. These scores are scaled down by a factor (qk_scale), typically the inverse square root of the dimension of the keys, to stabilize the gradients during training.

### 6. Attention Scores and Value Weighting
- Softmax is applied to the scaled attention scores to normalize them into a probability distribution. The resulting normalized scores are then used to weigh the values.

### 7. Enhancement with Local Positional Encodings
- After computing the initial attention outputs using the values, locally-enhanced positional encodings are added. This step incorporates specific positional information into the attention output, further enhancing its sensitivity to the positions within each local window.

### 8. Output Projection
- Finally, the attention outputs (enhanced with positional information) are passed through an output projection layer, which can also include dropout for regularization. The projected outputs are then typically used as inputs to subsequent layers or operations in the network.

### Conclusion
The LePEAttention mechanism enhances the traditional transformer attention by focusing on local regions within the input and enhancing these regions with locally specific positional encodings. This makes it particularly suited for tasks like image processing, where local spatial relationships are more informative and relevant than global relationships.

### 3. CSWinBlock Class
Description: Basic building block of the CSWin Transformer, containing a LePE attention layer and an MLP.
Parameters:
- dim: Dimension of the feature vectors.
- reso: Resolution of the input feature map.
- num_heads: Number of attention heads.
- split_size: Size of the split in the attention mechanism.
- mlp_ratio: Ratio of the hidden dimension to the input dimension in the MLP.
- qkv_bias: Boolean indicating if bias should be added to the QKV linear transformations.
- qk_scale: Scale factor for query-key interactions.

### 4. Merge_Block Class
Description: Performs down-sampling and channel-wise normalization to prepare features for the subsequent transformer stage.
Parameters:
- dim: Dimension of the input feature vectors.
- dim_out: Dimension of the output feature vectors.

### 5. CSWinTransformer Class
Description: The main transformer model which integrates multiple stages of CSWinBlock, along with SSF and CBAM modules for advanced feature handling.
Parameters:
- img_size: Size of the input images.
- patch_size: Size of each patch.
- in_chans: Number of input channels.
- num_classes: Number of classes for classification.
- embed_dim: Dimensionality of the embedding layer.
- depth: List detailing the number of blocks at each stage.
- split_size: List detailing the split size at each stage.
- num_heads: List detailing the number of heads at each stage.
- mlp_ratio: MLP ratio as in CSWinBlock.
- qkv_bias: Indicates if bias should be added to the QKV transformations.
- qk_scale: Scale for query-key interactions.
- drop_rate: Dropout rate for embeddings.
- attn_drop_rate: Dropout rate for attention weights.
- drop_path_rate: Rate for stochastic depth.
- hybrid_backbone: Optional CNN backbone for a hybrid architecture.

### 6. img2windows and windows2img Functions
Description: Helper functions to convert images to window partitions and vice versa, facilitating the processing of local features.
### 7. Model Registration Functions
Description: Registers different configurations of the CSWin Transformer for easy instantiation with or without pre-trained weights.



### 1. mlp_ratio
- Purpose: Controls the expansion of feature dimensions in MLP blocks to increase model capacity and enable learning of complex patterns.

### 2. qkv_bias
- Purpose: Adds a bias term in the linear transformations of queries, keys, and values to improve flexibility and fitting capabilities of the model.

### 3. qk_scale
- Purpose: Normalizes the dot products in attention mechanisms to stabilize training and prevent vanishing gradients.

### 4. embed_dim
- Purpose: Sets the size of feature embeddings, balancing the complexity of representations against computational efficiency and risk of overfitting.

### 5. drop_rate
- Purpose: Regularizes the model by randomly disabling a portion of the embeddings during training, promoting robustness and preventing overfitting.


The transforms.ToTensor() function is essential in preprocessing because it converts image data, typically stored in formats like PIL or NumPy arrays, into PyTorch tensors. This transformation standardizes the image data into a format suitable for input into neural networks, normalizing pixel values to the range [0, 1], which often leads to better performance during training. Additionally, transforming images to tensors allows for easier batch processing and GPU acceleration in PyTorch.